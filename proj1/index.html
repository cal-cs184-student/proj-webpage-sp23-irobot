<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Rasterizer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 1: Rasterizer</h1>
<h2 align="middle">Karthik Dharmarajan, Lawrence Yunliang Chen, CS184-irobot</h2>

<br><br>

    <div>

        <h2 align="middle">Overview</h2>
        <p>In this project, we first build a simple rasterizer that can render triangles. We then extend the functionality by implementing antialiasing by supersampling, computing hierarchical transforms and Barycentric coordinates. Finally, we implement texture mapping, with antialiasing techniques including bilinear interpolation and mipmap with trilinear interpolation. </p>

        <p>Putting together, our renderer can take in a simplified version of Scalable Vector Graphics (SVG) file and render it in a display window. We can use the mouse to translate the rendering on the screen as well as zoom in and zoom out; we can also use the keyboard to toggle different rendering settings, including the supersampling rate, the texture filtering methods on mipmap levels, and the texture filtering methods on pixels.</p>

        <p>We have learned a lot of intersting things from completing the project. First, we see how a rendering pipeline is implemented, and how the different methods and formulas we learned in lectures translate into different functions and bells and whistles that can be toggled on and off. We also notice the different effects of antialiasing methods for different images, and the trade offs between computation and image quality. Last but not least, we see how floating point precision issues can easily bring in artifacts, and how an inefficient implementation can easily slow down rendering.</p>

        <h2 align="middle">Section I: Rasterization</h2>

        <h3 align="middle">Part 1: Rasterizing single-color triangles</h3>

        <p>We first implement a function for rasterizing a single-color triangle. The basic idea is to iterate over each pixel in the triangle's bounding box, and check whether the pixel is inside the triangle. If it is, we set the color of the pixel to the triangle's color. </p>
        <p>In particular, the algorithm is as follows: </p>
        <ol>
            <li>
                Determine the bounding box of the triangle by finding the minimum and maximum x and y coordinates of the vertices.
            </li>
            <li>
                Iterate over each pixel (x, y) within the bounding box. Peform a point-in-triangle test to determine whether the coordinate (x+0.5, y+0.5) is inside the triangle. To do this, we can use the following formula:
                <br />

                <p>$$ {P=(x+0.5,y+0.5), \quad P_0=(x_0,y_0), P_1=(x_1,y_1), P_2=(x_2,y_2)} $$</p>
                <p>$$ {\vec{V_i}=P-P_i, \quad \vec{T_0}=P_1-P_0, \vec{T_1}=P_2-P_1, \vec{T_2}=P_0-P_2} $$</p>
                <p>$$ {\vec{N_i}=\bot(\vec{T_i})=\text{Rot}(90)(\vec{T_i})=(-(y_{i+1}-y_i),x_{i+1}-x_i)} $$</p>
                <p>$$ {L_i=\vec{V_i} \cdot \vec{N_i}=-(x+0.5-x_i)(y_{i+1}-y_i)+(y+0.5-y_i)(x_{i+1}-x_i) \geq 0 \text{ if } P \text{ is on the same side of } \vec{N_i}.}$$</p>
                <p>$$ If {L_0, L_1, L_2 \geq 0 \text{ or } L_0, L_1, L_2 \leq 0 \text{, then } P \text{ is inside the triangle.}} $$</p>

                Since the normal vector $\vec{N_i}$ is computed by rotating $\vec{T_i}$ in the counterclockwise direction, if the triangle is labeled in a counterclockwise order, then the point is inside the triangle if and only if $L_0, L_1, L_2 \geq 0$. If the triangle is labeled in a clockwise order, then the point is inside the triangle if and only if $L_0, L_1, L_2 \leq 0$.
            </li>
            <p>
                Finally, if the point is inside the triangle, set the color of the pixel to the triangle's color.
            </p>
        </ol>
        <br />
        <p>From the above logic, we can see that our algorithm is no worse than one that checks each sample within the bounding box of the triangle because we compute the bounding box first and only iterate over pixels in that bounding box.</p>

        <p>Here is a screenshot of the rendered <code>basic/test4.svg</code> with the default viewing parameters, showing different triangles. We can clear see jaggies:</p>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task1_test4.png" align="middle" width="800px" />
                        <figcaption align="middle"> <em>A png screenshot of basic/test4.svg with the default viewing parameters and with the pixel inspector centered on the bottom left vertex of the green triangle. We can see jaggies along the edges and the corners of the triangles.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <h4 align="left">Extra Credit</h4>
          <p>In our naive implementation, the order of the loops was not taken into account for cache performance, and there were redundant calculations within the loop that, while correct, were unnecessarily repeated calculations. We implemented two main optimizations which boost the speed of triangle rasterization. </p>
          <p>First, we modified the iteration over x-y in the for loops to have the outer loop be iterating over y and the inner loop over x. The cache access pattern is better in this case, as the stride within the inner loop in this setup is 1, while the stride in the inner loop in the worse configuration is $\text{width}$. Having a lower stride means more closely together sequential accesses, which are more likely to already be in the cache, and therefore would reduce memory read times. </p> 
          <p>The second optimization is a straight forward one that moves certain computations above the for loops.</p> 
          <p>To evaluate our optimizations compared to the baseline, we measure the nanoseconds of the execution of the <code>RasterizerImp::rasterize_triangle</code> function and the execution of the <code>Drawrend::redraw</code> function across 5 different images with varying sizes and numbers of triangles. In particular, for the <code>RasterizerImp::rasterize_triangle</code> calls, the duration was averaged over the number of triangles per image, while the millisecond durations for
            <code>Drawrend::redraw</code> were only from one call.</p>
            <table>
                <tr>
                    <th>Image Name</th>
                    <th>Unoptimized rasterize_triangle Execution Time (ns)</th>
                    <th>Optimized rasterize_triangle Execution Time (ns)</th>
                    <th>rasterize_triangle Speedup</th>
                    <th>Unoptimized redraw Execution Time (ms)</th>
                    <th>Optimized redraw Execution Time (ms)</th>
                    <th>redraw Speedup</th>
                </tr>
                <tr>
                    <td>basic/test3</td>
                    <td>8095.16</td>
                    <td>4615.03</td>
                    <td>1.75</td>
                    <td>15</td>
                    <td>11</td>
                    <td>1.36</td>
                </tr>
                <tr>
                    <td>basic/test5</td>
                    <td>443317</td>
                    <td>261783</td>
                    <td>1.69</td>
                    <td>2</td>
                    <td>1</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>basic/test6</td>
                    <td>13432.3</td>
                    <td>9018.55</td>
                    <td>1.49</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>illustration/02_hexes</td>
                    <td>21906.9</td>
                    <td>12911.2</td>
                    <td>1.70</td>
                    <td>2</td>
                    <td>1</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>illustration/04_suns</td>
                    <td>27667.8</td>
                    <td>15563.2</td>
                    <td>1.78</td>
                    <td>9</td>
                    <td>5</td>
                    <td>1.8</td>
                </tr>
            </table>
        </p>

        <p>From the above table, we can see that we are able to achieve a 1.7x speedup on average by optimizing our code. </p>

        <h3 align="middle">Part 2: Antialiasing triangles</h3>
        <p>
            Here, we extend our <code>rasterize_triangle</code> function to incorporate supersampling. The basic idea is to divide each pixel into smaller sub-pixels, and then compute the color of the pixel by averaging the colors of the samples. We implement this by first rasterizing an image that is of higher resolution, store it in the sample_buffer, and then downsampling the higher resolution image to the output resolution of the framebuffer.
        </p>
        <p>
            Supersampling is useful for antialiasing as it is equivalent to a 1 pixel-width box filter that attenuates frequencies whose period is less than or equal to 1 pixel-width. Instead of coloring a pixel whose center is inside the triangle the triangle color and not coloring it at all when the pixel center is outside the triangle, we color it an intermeidate value proportional to the area of the triangle inside the pixel. As such, supersampling can reduce jaggies and other visual artifacts such as moiré patterns caused by the limited resolution of the image.
        </p>
        <p>
            For our implementation, we sample at <code>sqrt(sample_rate) * sqrt(sample_rate)</code> grid locations distributed over each pixel area, and store all the higher resolution samples in the <code>sample_buffer</code>, which is a <code>std::vector</code> of <code>Color</code> values, similar to Task 1. But instead of storing <code>width * height</code> values, we now have <code>width * height * sample_rate</code> values, as each pixel now has <code>sample_rate</code> samples. We implemented the orders of elements in <code>sample_buffer</code> such that the <code>sample_rate</code> subpixels are contiguous, making it easier to access when downsampling. During downsampling (i.e., in <code>resolve_to_framebuffer</code>), we iterate through the pixels in the framebuffer and average the colors of the <code>sample_rate</code> samples stored contiguously in <code>sample_buffer</code>.
        </p>
        <p>In particular, the <code>RasterizerImp::rasterize_triangle</code> algorithm is as follows: </p>
        <ol>
            <li>
                Determine the bounding box of the triangle by finding the minimum and maximum x and y coordinates of the vertices.
            </li>
            <li>
                Iterate over each pixel $(x, y)$ within the bounding box. If the pixel center $(x+0.5, y+0.5)$ is outside the triangle, skip it.
                <br />
                <ul>
                    <li>
                        For each pixel, iterate over each subpixel $(x', y')$ within the pixel, where $(x', y') = (x + \frac{i+0.5}{\sqrt{\text{sample_rate}}}, y + \frac{j+0.5}{\sqrt{\text{sample_rate}}})$</code> and $i$ and $j$ both range from 0 to $\sqrt{\text{sample_rate}}-1$.
                        <br />
                        <ul>
                            <li>
                                Check whether the subpixel center is inside the triangle. If yes, store the color of the subpixel in the <code>sample_buffer</code>.
                            </li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ol>
        <p>The main modifications we make to the rasterization pipeline from Task 1 is that 1) we increase the length of the <code>sample_buffer</code> by a factor of <code>sample_rate</code> and fill in <code>sample_rate</code> colors in the vector as we iterate through the pixels, and 2) we add two inner for loops to iterate through the subpixels (horizontally and vertically) and perform a point-in-triangle test for each subpixel center instead of for each pixel.</p>

        <p>The <code>RasterizerImp::resolve_to_framebuffer</code> algorithm now looks like follows: </p>
        <ul>
            <li>
                Iterate over each pixel of the frame buffer.
                <ul>
                    <li>
                        For each pixel, extract the <code>sample_rate</code> samples from the <code>sample_buffer</code> and average them to get the final color of the pixel.
                        <br />
                    </li>
                </ul>
            </li>
        </ul>
        <p>The main modification is to the <code>sample_rate</code> samples and perform averaging instead of taking only 1 sample from the <code>sample_buffer</code>.</p>

        <br />
        <p>Below, we show png screenshots of <code>basic/test4.svg</code> with the default viewing parameters and sample rates 1, 4, and 16 to compare them side-by-side. The pixel inspector is positioned over the top right corner of the red triangle.</p>
        <div align="middle">
            <table style="width=150%">
                <tr>
                    <td>
                        <img src="images/task2_supersampling_1.png" align="middle" width="330px" />
                        <figcaption align="middle"> <em>Supersampling with sample rate = 1.</em></figcaption>
                    </td>
                    <td>
                        <img src="images/task2_supersampling_4.png" align="middle" width="330px" />
                        <figcaption align="middle"> <em>Supersampling with sample rate = 4.</em></figcaption>
                    </td>
                    <td>
                        <img src="images/task2_supersampling_16.png" align="middle" width="330px" />
                        <figcaption align="middle"> <em>Supersampling with sample rate = 16.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <p>From the comparison, we can see that the default sample rate of 1 produces significant jaggies along the edges of the triangle. Increasing the sample rate to 4 or 16 reduces the jaggies. This antialiasing effect is expected because supersampling is equivalent to a 1 pixel-width box low-pass filter, which blurs the edges of the triangle. The more samples we take, the smoother the edges of the triangle become. In places such as a skinny triangle, the edges pass through many pixels partially, and so filling the pixels with intermediate values creates a smoother transition.</p>
        </p>

        <br />

        <h3 align="middle">Part 3: Transforms</h3>
        <p>
            In this part, we implement some basic 2D transform functions in <code>transforms.cpp</code> to apply 2D transformations to the vertices of a triangle. We implement the 2D transformations with the following formula:
        </p>
        <ul>
            <li>
                Translation: $ {\begin{bmatrix}1 & 0 & d_x \\0 & 1 & d_y\\0 & 0 & 1\end{bmatrix}} $
            </li>
            <li>
                Rotation: $ {\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\\sin\theta & \cos\theta & 0\\0 & 0 & 1\end{bmatrix}} $
            </li>
            <li>
                Scaling: $ {\begin{bmatrix}s_x & 0 & 0 \\0 & s_y & 0\\0 & 0 & 1\end{bmatrix}} $
            </li>
        </ul>


        <p>We modified <code>svg/transforms/robot.svg</code> and created a cubeman running. Below, we show the rendering of the original svg and the modified svg side by side. In particular, we changed the colors of the head, torso, and legs of the cubeman. We also rotated the head, the arms, and the legs to make them consistent with a running motion. </p>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task3_robot.png" align="middle" width="400px" />
                        <figcaption align="middle">robot.svg: Cubeman is in a T-pose.</figcaption>
                    </td>
                    <td>
                        <img src="images/task3_myrobot.png" align="middle" width="400px" />
                        <figcaption align="middle">my_robot.svg: Cubeman is running.</figcaption>
                    </td>
                </tr>
            </table>
        </div>


        <h2 align="middle">Section II: Sampling</h2>

        <h3 align="middle">Part 4: Barycentric coordinates</h3>
        <p>In this part, we implement the <code>rasterize_interpolated_color_triangle</code> function, which uses Barycentric interpolation to draw triangles with non-uniform colors. </p>

        <p>Barycentric coordinates are a way to represent the coordinates of a point within a triangle as a convex combination of the triangle's vertices' coordinates. Specifically, in 2D, the barycentric coordinates of a point within a nondegenerate triangle $ABC$ are defined as the weights $(\alpha, \beta, \gamma)$, with $\alpha + \beta + \gamma = 1$ such that $\vec{P} = \alpha \vec{A} + \beta \vec{B} + \gamma \vec{C}$. The barycentric coordinates of a point can be thought of as the relative distances between the point and each vertex. For example, a point that is closer to the green vertex will have a larger weight for the green vertex in its barycentric coordinates.</p>

        <p>Barycentric coordinates are useful because they provide a coordinate system of points relative to the triangle frame that is invariant to rigid transform of the triangle, and can be used to smoothly (linearly) interpolate values within the triangle from the triangle's vertices. This is useful in rendering colors and textures.</p>

        <p>To illustrate this concept, let's consider the following triangle:</p>

        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task4_barycentric_illustration.png" align="middle" width="800px" />
                        <figcaption align="middle"> <em>A smoothly blended triangle with red, green, and blue assigned to each vertex, using barycentric interpolation.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>The vertices of this triangle are colored red, green, and blue. Any point within this triangle can be represented as a linear combination of the vertices, where the coefficients of the linear combination are the barycentric coordinates of the point. For each pixel inside the triangle, we can compute its barycentric coordinates, and then use those coordinates to compute the color of the pixel as a weighted sum of the vertex colors. The result is a smoothly blended color triangle.</p>

        <p>Below, we show a png screenshot of <code>svg/basic/test7.svg</code> with default viewing parameters and sample rate 1. We can see the barycentric interpolation results in smooth color gradients.</p>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task4_test7.png" align="middle" width="800px" />
                        <figcaption align="middle"> <em>A png screenshot of <code>svg/basic/test7.svg</code>.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>


        <h3 align="middle">Part 5: "Pixel sampling" for texture mapping</h3>

        <p>Pixel sampling refers to determining the color of a pixel on the screen by sampling the colors from the texture map. In texture mapping, a 2D image called a texture is applied to a triangle or a 3D model in general to give it the appearance of surface details and patterns. For triangle elements, each vertex is associated with a position in the texture image. To determine the color of the patch on the surface corresponding to a pixel on the screen, we need to sample and/or interpolate the colors from the corresponding pixels on the texture.</p>

        <p>In this part, we implement two main methods of pixel sampling: nearest-neighbor and bilinear filtering. Nearest-neighbor sampling simply chooses the color of the texel closest to the center of the pixel being rendered. This method is simple and fast, but it can result in a blocky or pixelated appearance. In particular, when magnified, we see jaggies, and when minified, we may see moiré patterns.</p>

        <p>In bilinear sampling, the color of the pixel on the texture is interpolated based on the four nearest pixels to the point being sampled. This method produces a smoother and visually pleasing result than nearest neighbor sampling, albeit at the cost of being more computationally expensive than nearest neighbor sampling.</p>
        
        <p>Here, we describe our implemntation for nearest and bilinear sampling:</p>
        <ul>
            <li>First, given the screen pixel coordinates, we use Barycentric coordinates to convert it into the texture coordinates. We scale them by $(\text{width} - 1)$ and $(\text{height} - 1)$ of the mipmap for the level we desire to use, to convert it into the texel indices.</li>
            <li>For both nearest and bilinear pixel sampling, we find the four nearest texels in the texture image for a given sampled location. Let the desired sample coordinate at a specific mipmap level be $(x_0, y_0)$. The four closest points to the input point can be accomplished by taking the floor and ceiling of $(x_0, y_0)$, yielding $x_1$, $x_2$, $y_1$, and $y_2$ in the coordinates of a mipmap level, where $x_2 = x_1 + 1$ and $y_2 = y_1 + 1$.</li>
            <li><b>Nearest sampling:</b> For each pixel in the triangle, we find the texel in the texture image that is closest to the center of the pixel (i.e., among $x_1$, $x_2$, $y_1$, and $y_2$, the one closest to $x_0$ and  $y_0$ are taken respectively). We then use the color of that texel as the color of the pixel.</li>
            <li><b>Bilinear sampling:</b> Instead of using the nearest neighbor, we use bilinear interpolation to compute the color of the pixel:</li>
            $$\begin{align*}
                \text{Color}_1 &= \text{Color} (x_1, y_1) + (x_0 - x_1) \times (\text{Color} (x_2, y_1) - \text{Color} (x_1, y_1)) \\
                \text{Color}_2 &= \text{Color} (x_1, y_2) + (x_0 - x_1) \times (\text{Color} (x_2, y_2) - \text{Color} (x_1, y_2)) \\
                \text{Color}(x_0, y_0) &= \text{Color}_1 + (y_0 - y_1) \times (\text{Color}_2 - \text{Color}_1)
            \end{align*}$$
        </ul>
        <br>
        <p>Below, we show an example of where bilinear sampling clearly defeats nearest sampling using <code>svg/texmap/test4.svg</code>.</p>

        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task5_nearest_1.png" align="middle" width="400px" />
                        <figcaption align="middle"><em><code>texmap/test4.svg</code> using nearest pixel sampling and sampling rate of 1.</em></figcaption>
                    </td>
                    <td>
                        <img src="images/task5_nearest_16.png" align="middle" width="400px" />
                        <figcaption align="middle"><em><code>texmap/test4.svg</code> using nearest pixel sampling and sampling rate of 16.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task5_bilinear_1.png" align="middle" width="400px" />
                        <figcaption align="middle"><em><code>texmap/test4.svg</code> using bilinear pixel sampling and sampling rate of 1.</em></figcaption>
                    </td>
                    <td>
                        <img src="images/task5_bilinear_16.png" align="middle" width="400px" />
                        <figcaption align="middle"><em><code>texmap/test4.svg</code> using bilinear pixel sampling and sampling rate of 16.</em></figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <p>In this example of the UC Berkeley seal, the border surrounding the various letters with nearest pixel sampling and a supersampling rate of 1 results in a very blocky and pixelated look. When increasing the supersampling rate to 16, the jaggies are reduced, but there are still some artifacts that don't have a smooth border.</p>
            
        <p>On the other hand, bilinear sampling has smooth borders even at a supersampling rate of 1, and there is not a significant difference when increasing the sampling rate to 16. This is because bilinear sampling takes into account the four nearest pixels to the point being sampled, and thus is able to produce a smoother transition between the colors of the pixels, achieving similar results to nearest pixel sampling at a higher supersampling rate.</p>

        <p>The difference between the two methods is most noticeable when the texture is magnified or minified significantly. When the texture is magnified, nearest neighbor sampling produces blocky, pixelated textures, while bilinear sampling produces smoother, more natural-looking textures. Conversely, when the texture is minified, nearest neighbor sampling may produce sharper, more defined textures, but also often moiré patterns, while bilinear sampling may produce smoother textures that look blurrier in some cases.</p>

        <h3 align="middle">Part 6: "Level sampling" with mipmaps for texture mapping</h3>
        <p>Level sampling is a technique used in texture mapping to improve performance by using a mipmap. This is motivated by the fact that for minification, one pixel sample corresponds to multiple texel samples, and this is inefficient to compute and may also lead to aliasing. In level sampling, instead of using a single texture, we compute a set of pre-filtered textures at different resolutions, called mipmaps. When rendering a texture mapped object, the appropriate level of the mipmap is chosen based on the ratio of the pixel sampling rate and the texel sampling rate, and then pixel sampling method is applied to that level.</p>
        <p>We implemented level sampling for texture mapping as follows:</p>
        <ul>
            <li>To determine what mipmap level to sample from, we determine $\frac{du}{dx}, \frac{dv}{dx}, \frac{dv}{dx}, \frac{dv}{dy}$, where $(u, v)$ are in texture coordinates and $x,y$ are in screen coordinates.</li>
            <li>To calculate those derivatives, we first get the $(u, v)$ coordinates for $(x, y), (x + 1, y), (x, y + 1)$ using Barycentric interpolation, where $(x, y)$ is the screen coordinates we sample (e.g., $(i+0.5, j+0.5)$ for naive rasterization and corresponding positions for supersampling). It is possible that $(x + 1, y)$ and $(x, y + 1)$ are outside of a triangle, and in such cases we clamp them back and use the original $(x, y)$. The resulting coordinates in texture space can then be subtracted and scaled by $(\text{width} - 1)$ in the case of a derivative with respect to $x$ or $(\text{height} - 1)$ in the case of a derivative with respect to $y$.</li>
            <li>Then, we compute the sampling level $D = \log_2(\max(\sqrt{(\frac{du}{dx})^2 + (\frac{du}{dx})^2}, \sqrt{(\frac{dv}{dx})^2 + (\frac{dv}{dx})^2}))$, where $D$ is the desired mipmap level.</li>
            <li>For nearest level sampling, we round $D$ to the nearest valid integer beteen level 0 and the highest mipmap level we have.</li>
            <li>For trilinear interpolation, we treat $D$ as a continuous level. We take the floor and ceiling of $D$, compute the texel samples at level $\text{floor}(D)$ and $\text{ceil}(D)$ using bilinear sampling, and then interpolate the colors between the two levels for a final result. We also perform clamping on the levels returned to make sure that level is greater than or equal to 0 and lower than the maximum number mipmap level.</li>
        </ul>
        <p>We now can choose our sampling technique by selecting pixel sampling, level sampling, or the number of samples per pixel in a combinatorial way. There are tradeoffs between speed, memory usage, and antialiasing power between the three various techniques.</p>
        <p>Here, we perform a simple complexity analysis. Let W be the width of the image, H be the height of the image. We assume that only one of the methods is used at a given time. </p>
        <ul>
          <li>For pixel sampling, the space complexity of is $O(WH)$, since it is possible to have the rasterization cover the entire area of the screen. The time complexity of pixel sampling is also $O(WH)$, as for each pixel, there is a constant amount of work done in both the nearest sampling and bilinear sampling cases.</li>
          <ul>
            <li>The computation for bilinear pixel sampling is a constant multiplier of nearest pixel sampling, since for nearest sampling, we only need to retrieve the color of the nearest texel, while for bilinear sampling, we need to retrieve the color of 4 texels and perform bilinear interpolation. </li>
          </ul>
          <li>For level sampling, the space complexity is also $O(WH)$, because the total memory used to represent the mipmap is $\frac{1}{3}WH$. The time complexity of level sampling is $O(WH)$ in total, as for each pixel, there is a constant time set of operations performed. </li>
          <ul>
            <li>The computation for linear level sampling is a constant multiplier of nearest level sampling, since requires computing the color on 2 mipmap levels instead of 1 and needs to perform a linear interpolation. </li>
          </ul>
          <li>For supersampling with sample rate $S$, the space complexity of is $O(WHS)$ in our method, as we need to store a high resolution buffer which contains S values for each pixel in the buffer. Since super sampling involves computing $S$ samples for each of the $WH$ pixels, the time complexity is also $O(WHS)$ (downsampling involves visiting $O(WHS)$ elements). </li>
        </ul>
        <p>In our rendering experiments, we find that:</p>
        <ul>
          <li>Bilinear sampling can be effective at smoothing certain aliased regions, but it doesn't perform as well as supersampling in certain cases, such as for <code>basic/test4.svg</code>, where bilinear interpolation doesn't make all triangles look like one piece (the pink triangle is fragmented). </li>
          <li>Level sampling can be helpful in certain cases, such as when the texture is minified, but it can also make certain parts of the image too blurry.</li>
          <li>Supersampling has the strongest antialiasing results, such as extra sharp corners or missing pixels, but the computation is also highest, especially when considering two other antialiasing methods. </li>
        </ul>
        <p>Overall, pixel sampling is the fastest and uses the least memory, but can produce jaggies or blocky artifacts when the texture is magnified or minified. Level sampling is slower than pixel sampling and faster than supersampling, but uses slightly more memory to store the mipmaps. Level sampling provides antialiasing power by selecting the appropriate mipmap level for the texture size, but can also make certain aspects of the image too blurry. In contrast, always using level 0 texture can be computationally inefficient when the object is minified and can result in aliasing. Finally, supersampling requires much more memory and can be computationally most expensive, but it can provide high-quality antialiasing.</p>
        
        <p>Below, we use a png file we find ourselves as the texture map for <code>svg/texmap/test1.svg</code>, and show four versions of the image, using the combinations of <code>L_ZERO</code> and <code>P_NEAREST</code>, <code>L_ZERO</code> and <code>P_LINEAR</code>, <code>L_NEAREST</code> and <code>P_NEAREST</code>, as well as <code>L_NEAREST</code> and <code>P_LINEAR</code>.</p>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task6_zero_nearest.png" align="middle" width="400px" />
                        <figcaption align="middle">Image of San Francisco using mipmap level 0 and nearest pixel sampling.</figcaption>
                    </td>
                    <td>
                        <img src="images/task6_zero_linear.png" align="middle" width="400px" />
                        <figcaption align="middle">Image of San Francisco using mipmap level 0 and bilinear pixel sampling.</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <div align="middle">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="images/task6_nearest_nearest.png" align="middle" width="400px" />
                        <figcaption align="middle">Image of San Francisco using nearest mipmap level and nearest pixel sampling.</figcaption>
                    </td>
                    <td>
                        <img src="images/task6_nearest_linear.png" align="middle" width="400px" />
                        <figcaption align="middle">Image of San Francisco using nearest mipmap level and bilinear pixel sampling.</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>We selected an image of the city of San Francisco for showing four different combinations of mipmap level selection and pixel sampling methods. </p>
        <ul>
          <li>Using zero level mipmap and nearest pixel sampling shows an image with the most aliasing, as some edges are sharp, and some lines which are straight in real life are curving. </li>
          <li>Adding bilinear sampling to that ends up smoothing the lines, though an interesting diagonal pattern is showing up on one of the buildings. </li>
          <li>When switching to nearest mipmap level and nearest pixel sampling, there is still some aliasing, but the background of the bay and hills becomes much smoother. </li>
          <li>Adding bilinear interpolation to this results in the smoothest zoomed in area. Although there are still some aliasing artifacts, it is the best among the four.</li>
        </ul>
    </div>

    <p>Here is the link to this webpage:</p>
    <a href="https://cal-cs184-student.github.io/proj-webpage-sp23-irobot/proj1/index.html">https://cal-cs184-student.github.io/proj-webpage-sp23-irobot/proj1/index.html</a>
</body>
</html>
